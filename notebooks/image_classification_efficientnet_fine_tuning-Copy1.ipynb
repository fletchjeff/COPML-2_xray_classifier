{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3omXeTSNJrzu"
   },
   "source": [
    "# Image classification via fine-tuning with EfficientNet\n",
    "\n",
    "**Author:** [Yixing Fu](https://github.com/yixingfu)<br>\n",
    "**Date created:** 2020/06/30<br>\n",
    "**Last modified:** 2020/07/16<br>\n",
    "**Description:** Use EfficientNet with weights pre-trained on imagenet for Stanford Dogs classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8pHlbKWJrzy"
   },
   "source": [
    "## Introduction: what is EfficientNet\n",
    "\n",
    "EfficientNet, first introduced in [Tan and Le, 2019](https://arxiv.org/abs/1905.11946)\n",
    "is among the most efficient models (i.e. requiring least FLOPS for inference)\n",
    "that reaches State-of-the-Art accuracy on both\n",
    "imagenet and common image classification transfer learning tasks.\n",
    "\n",
    "The smallest base model is similar to [MnasNet](https://arxiv.org/abs/1807.11626), which\n",
    "reached near-SOTA with a significantly smaller model. By introducing a heuristic way to\n",
    "scale the model, EfficientNet provides a family of models (B0 to B7) that represents a\n",
    "good combination of efficiency and accuracy on a variety of scales. Such a scaling\n",
    "heuristics (compound-scaling, details see\n",
    "[Tan and Le, 2019](https://arxiv.org/abs/1905.11946)) allows the\n",
    "efficiency-oriented base model (B0) to surpass models at every scale, while avoiding\n",
    "extensive grid-search of hyperparameters.\n",
    "\n",
    "A summary of the latest updates on the model is available at\n",
    "[here](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet), where various\n",
    "augmentation schemes and semi-supervised learning approaches are applied to further\n",
    "improve the imagenet performance of the models. These extensions of the model can be used\n",
    "by updating weights without changing model architecture.\n",
    "\n",
    "## B0 to B7 variants of EfficientNet\n",
    "\n",
    "*(This section provides some details on \"compound scaling\", and can be skipped\n",
    "if you're only interested in using the models)*\n",
    "\n",
    "Based on the [original paper](https://arxiv.org/abs/1905.11946) people may have the\n",
    "impression that EfficientNet is a continuous family of models created by arbitrarily\n",
    "choosing scaling factor in as Eq.(3) of the paper.  However, choice of resolution,\n",
    "depth and width are also restricted by many factors:\n",
    "\n",
    "- Resolution: Resolutions not divisible by 8, 16, etc. cause zero-padding near boundaries\n",
    "of some layers which wastes computational resources. This especially applies to smaller\n",
    "variants of the model, hence the input resolution for B0 and B1 are chosen as 224 and\n",
    "240.\n",
    "\n",
    "- Depth and width: The building blocks of EfficientNet demands channel size to be\n",
    "multiples of 8.\n",
    "\n",
    "- Resource limit: Memory limitation may bottleneck resolution when depth\n",
    "and width can still increase. In such a situation, increasing depth and/or\n",
    "width but keep resolution can still improve performance.\n",
    "\n",
    "As a result, the depth, width and resolution of each variant of the EfficientNet models\n",
    "are hand-picked and proven to produce good results, though they may be significantly\n",
    "off from the compound scaling formula.\n",
    "Therefore, the keras implementation (detailed below) only provide these 8 models, B0 to B7,\n",
    "instead of allowing arbitray choice of width / depth / resolution parameters.\n",
    "\n",
    "## Keras implementation of EfficientNet\n",
    "\n",
    "An implementation of EfficientNet B0 to B7 has been shipped with tf.keras since TF2.3. To\n",
    "use EfficientNetB0 for classifying 1000 classes of images from imagenet, run:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "model = EfficientNetB0(weights='imagenet')\n",
    "```\n",
    "\n",
    "This model takes input images of shape (224, 224, 3), and the input data should range\n",
    "[0, 255]. Normalization is included as part of the model.\n",
    "\n",
    "Because training EfficientNet on ImageNet takes a tremendous amount of resources and\n",
    "several techniques that are not a part of the model architecture itself. Hence the Keras\n",
    "implementation by default loads pre-trained weights obtained via training with\n",
    "[AutoAugment](https://arxiv.org/abs/1805.09501).\n",
    "\n",
    "For B0 to B7 base models, the input shapes are different. Here is a list of input shape\n",
    "expected for each model:\n",
    "\n",
    "| Base model | resolution|\n",
    "|----------------|-----|\n",
    "| EfficientNetB0 | 224 |\n",
    "| EfficientNetB1 | 240 |\n",
    "| EfficientNetB2 | 260 |\n",
    "| EfficientNetB3 | 300 |\n",
    "| EfficientNetB4 | 380 |\n",
    "| EfficientNetB5 | 456 |\n",
    "| EfficientNetB6 | 528 |\n",
    "| EfficientNetB7 | 600 |\n",
    "\n",
    "When the model is intended for transfer learning, the Keras implementation\n",
    "provides a option to remove the top layers:\n",
    "```\n",
    "model = EfficientNetB0(include_top=False, weights='imagenet')\n",
    "```\n",
    "This option excludes the final `Dense` layer that turns 1280 features on the penultimate\n",
    "layer into prediction of the 1000 ImageNet classes. Replacing the top layer with custom\n",
    "layers allows using EfficientNet as a feature extractor in a transfer learning workflow.\n",
    "\n",
    "Another argument in the model constructor worth noticing is `drop_connect_rate` which controls\n",
    "the dropout rate responsible for [stochastic depth](https://arxiv.org/abs/1603.09382).\n",
    "This parameter serves as a toggle for extra regularization in finetuning, but does not\n",
    "affect loaded weights. For example, when stronger regularization is desired, try:\n",
    "\n",
    "```python\n",
    "model = EfficientNetB0(weights='imagenet', drop_connect_rate=0.4)\n",
    "```\n",
    "The default value is 0.2.\n",
    "\n",
    "## Example: EfficientNetB0 for Stanford Dogs.\n",
    "\n",
    "EfficientNet is capable of a wide range of image classification tasks.\n",
    "This makes it a good model for transfer learning.\n",
    "As an end-to-end example, we will show using pre-trained EfficientNetB0 on\n",
    "[Stanford Dogs](http://vision.stanford.edu/aditya86/ImageNetDogs/main.html) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WyDk64XIJrzz"
   },
   "outputs": [],
   "source": [
    "# IMG_SIZE is determined by EfficientNet model choice\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uta9VZCuJrzz"
   },
   "source": [
    "## Setup and data loading\n",
    "\n",
    "This example requires TensorFlow 2.3 or above.\n",
    "\n",
    "To use TPU, the TPU runtime must match current running TensorFlow\n",
    "version. If there is a mismatch, try:\n",
    "\n",
    "```python\n",
    "from cloud_tpu_client import Client\n",
    "c = Client()\n",
    "c.configure_tpu_version(tf.__version__, restart_type=\"always\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcusolver.so.10': File exists\n"
     ]
    }
   ],
   "source": [
    "!ln -s /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcusolver.so.11.0.1.105 /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcusolver.so.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "A1N9sN5kJrzz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# try:\n",
    "#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "#     print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
    "#     tf.config.experimental_connect_to_cluster(tpu)\n",
    "#     tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "# except ValueError:\n",
    "#     print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
    "strategy = tf.distribute.MirroredStrategy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hH9qYJTnJrz0"
   },
   "source": [
    "### Loading data\n",
    "\n",
    "Here we load data from [tensorflow_datasets](https://www.tensorflow.org/datasets)\n",
    "(hereafter TFDS).\n",
    "Stanford Dogs dataset is provided in\n",
    "TFDS as [stanford_dogs](https://www.tensorflow.org/datasets/catalog/stanford_dogs).\n",
    "It features 20,580 images that belong to 120 classes of dog breeds\n",
    "(12,000 for training and 8,580 for testing).\n",
    "\n",
    "By simply changing `dataset_name` below, you may also try this notebook for\n",
    "other datasets in TFDS such as\n",
    "[cifar10](https://www.tensorflow.org/datasets/catalog/cifar10),\n",
    "[cifar100](https://www.tensorflow.org/datasets/catalog/cifar100),\n",
    "[food101](https://www.tensorflow.org/datasets/catalog/food101),\n",
    "etc. When the images are much smaller than the size of EfficientNet input,\n",
    "we can simply upsample the input images. It has been shown in\n",
    "[Tan and Le, 2019](https://arxiv.org/abs/1905.11946) that transfer learning\n",
    "result is better for increased resolution even if input images remain small.\n",
    "\n",
    "For TPU: if using TFDS datasets,\n",
    "a [GCS bucket](https://cloud.google.com/storage/docs/key-terms#buckets)\n",
    "location is required to save the datasets. For example:\n",
    "\n",
    "```python\n",
    "tfds.load(dataset_name, data_dir=\"gs://example-bucket/datapath\")\n",
    "```\n",
    "\n",
    "Also, both the current environment and the TPU service account have\n",
    "proper [access](https://cloud.google.com/tpu/docs/storage-buckets#authorize_the_service_account)\n",
    "to the bucket. Alternatively, for small datasets you may try loading data\n",
    "into the memory and use `tf.data.Dataset.from_tensor_slices()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.2.0-py3-none-any.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 18.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 1.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in ./.local/lib/python3.6/site-packages (from tensorflow_datasets) (3.15.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/site-packages (from tensorflow_datasets) (2.25.1)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 58.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from tensorflow_datasets) (1.19.4)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: termcolor in ./.local/lib/python3.6/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.6/site-packages (from tensorflow_datasets) (3.7.4.3)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 623 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/site-packages (from tensorflow_datasets) (20.3.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/site-packages (from tensorflow_datasets) (0.8)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.1.2-py3-none-any.whl (25 kB)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.30.0-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 553 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py in ./.local/lib/python3.6/site-packages (from tensorflow_datasets) (0.12.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow_datasets) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.11.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: zipp>=0.4 in ./.local/lib/python3.6/site-packages (from importlib-resources->tensorflow_datasets) (3.4.1)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 75.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: future, promise\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=6c33e6d0893fe35001034d50242664e834a8693126af912f03492a1e1699a432\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=27b6c1700333f0662f6810440579253c65925bd0ef2611e5e5a6e263a03c3813\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "Successfully built future promise\n",
      "Installing collected packages: googleapis-common-protos, tqdm, tensorflow-metadata, promise, importlib-resources, future, dill, tensorflow-datasets\n",
      "Successfully installed dill-0.3.3 future-0.18.2 googleapis-common-protos-1.53.0 importlib-resources-5.1.2 promise-2.3 tensorflow-datasets-4.2.0 tensorflow-metadata-0.30.0 tqdm-4.60.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "0QoW3BtaJrz0"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dataset_name = \"stanford_dogs\"\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    dataset_name, split=[\"train\", \"test\"], with_info=True, as_supervised=True\n",
    ")\n",
    "NUM_CLASSES = ds_info.features[\"label\"].num_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00ixin4sJrz0"
   },
   "source": [
    "When the dataset include images with various size, we need to resize them into a\n",
    "shared size. The Stanford Dogs dataset includes only images at least 200x200\n",
    "pixels in size. Here we resize the images to the input size needed for EfficientNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "hRf89kdHJrz0"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    <ipython-input-5-ab1b0e04ad93>:2 None  *\n        lambda image, label: (tf.image.resize(image, size), label))\n    /home/cdsw/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    /home/cdsw/.local/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py:1647 resize_images_v2\n        skip_resize_if_same=False)\n    /home/cdsw/.local/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py:1329 _resize_images_common\n        raise ValueError('\\'size\\' must be a 1-D Tensor of 2 elements: '\n\n    ValueError: 'size' must be a 1-D Tensor of 2 elements: new_height, new_width\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ab1b0e04ad93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     \"\"\"\n\u001b[1;32m   1804\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4206\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4207\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4208\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   4209\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3052\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3516\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3517\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3518\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3519\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3453\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3454\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3455\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-5-ab1b0e04ad93>:2 None  *\n        lambda image, label: (tf.image.resize(image, size), label))\n    /home/cdsw/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    /home/cdsw/.local/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py:1647 resize_images_v2\n        skip_resize_if_same=False)\n    /home/cdsw/.local/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py:1329 _resize_images_common\n        raise ValueError('\\'size\\' must be a 1-D Tensor of 2 elements: '\n\n    ValueError: 'size' must be a 1-D Tensor of 2 elements: new_height, new_width\n"
     ]
    }
   ],
   "source": [
    "size = (IMG_SIZE, IMG_SIZE)\n",
    "ds_train = ds_train.map(lambda image, label: (tf.image.resize(image, size), label))\n",
    "ds_test = ds_test.map(lambda image, label: (tf.image.resize(image, size), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7EJqymiJrz0"
   },
   "source": [
    "### Visualizing the data\n",
    "\n",
    "The following code shows the first 9 images with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((224, 224, 3), ()), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "AqRlWRM2Jrz1"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (64, 224, 224, 3) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-77fcb4903ca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2728\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2730\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m   2731\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5523\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    709\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 710\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (64, 224, 224, 3) for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAG0AAABjCAYAAACR8o4mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEjElEQVR4nO2dQWgcVRjHf39Tq5CDBZuDaKEWiyEHD+kiOZWCCG0PyUEP6aVGKqFo8Sx4EHqRnoSiWIIGrYda7CmCIoJCT63dgNZWUbaCGAk0rZKLUA18HmaSrpvd7HR9k9mPfD8YmJn33rwv+TGzb1/yvZGZEfjivqoDCO6dkOaQkOaQkOaQkOaQkOaQrtIkzUq6Kelah3JJOi2pIemqpNH0YQbNFLnTPgAOblB+CNibb9PAu/8/rGAjukozs4vAHxtUmQDOWsYlYIekR1IFGKwnxWfao8BvTccL+bmgJLZtZmeSpskeoQwODu4bHh7ezO77ivn5+VtmNtRL2xTSfgd2NR0/lp9bh5nNADMAtVrN6vV6gu59IunXXtumeDzOAUfzUeQYsGxmiwmuG3Sg650m6RxwANgpaQF4A7gfwMzOAJ8Bh4EG8BfwYlnBBhldpZnZkS7lBrySLKKgKzEj4pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pCQ5pBC0iQdlPRTnoP2WpvyKUlLkr7Nt5fShxqsUuQ/jAeAd4BnyTJirkiaM7MfWqqeN7MTJcQYtFDkTnsaaJjZL2b2N/AxWU5aUBFFpBXNP3suT9+9IGlXm/IgEakGIp8Cu83sKeBL4MN2lSRNS6pLqi8tLSXqeutRRFrX/DMzu21md/LD94B97S5kZjNmVjOz2tBQT/l0AcWkXQH2Snpc0nZgkiwnbY2WHOtx4Md0IQatFEl1WpF0AvgCGABmzey6pJNA3czmgFcljQMrZEn1UyXGvOVRVUsHRvqu5s2s1kvbmBFxSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSKr8tAcknc/LL0vanTzSYI0iL71bzU87BIwARySNtFQ7BvxpZk8AbwGnUgca3CVVftoEdzNlLgDPSFK6MINmUuWnrdUxsxVgGXg4RYDBeip7fxpwp9N7RjeJncCtCvt/steGRaQVeT/aap0FSduAh4DbrRdqfn+apHqvCQgp6If+e22bJD8tP34h338e+MqqSsfZAqTKT3sf+EhSgyw/bbLMoLc8ZlbJBkxX1bf3/itLKgx6J6axHFK6tKqnwKpcIkrSrKSbnb7a5C+/PZ3HdlXSaKELl/zcHgBuAHuA7cB3wEhLnZeBM/n+JNlyTZvZ/xTwdkk//35gFLjWofww8DkgYAy4XOS6Zd9pVU+BVbpElJldJBtNd2ICOGsZl4AdLct7tKVsaVVPgfX7ElFF4/sPMRApuERUP1G2tHuZAmOjKbCy+reCS0SVRJHfzzrKllb1FFi/LxE1BxzNR5FjwLKZLXZtVebosWmE9DPZKO71/NxJYDzffxD4BGgA3wB7Nrn/N4HrZCPLr4HhhH2fAxaBf8g+r44Bx4HjebnI/sB8A/geqBW5bsyIOCQGIg4JaQ4JaQ4JaQ4JaQ4JaQ4JaQ4JaQ75F4dklSJNDnL9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def format_label(label):\n",
    "    string_label = label_info.int2str(label)\n",
    "    return string_label.split(\"-\")[1]\n",
    "\n",
    "\n",
    "label_info = ds_info.features[\"label\"]\n",
    "for i, (image, label) in enumerate(ds_train.take(9)):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "    plt.title(\"{}\".format(format_label(label)))\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHhSDI5VJrz1"
   },
   "source": [
    "### Data augmentation\n",
    "\n",
    "We can use preprocessing layers APIs for image augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CU-VjwWLJrz1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "img_augmentation = Sequential(\n",
    "    [\n",
    "        preprocessing.RandomRotation(factor=0.15),\n",
    "        preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        preprocessing.RandomFlip(),\n",
    "        preprocessing.RandomContrast(factor=0.1),\n",
    "    ],\n",
    "    name=\"img_augmentation\",\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwtJB7iKJrz1"
   },
   "source": [
    "This `Sequential` model object can be used both as a part of\n",
    "the model we later build, and as a function to preprocess\n",
    "data before feeding into the model. Using them as function makes\n",
    "it easy to visualize the augmented images. Here we plot 9 examples\n",
    "of augmentation result of a given figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2RsZ5RXJrz1"
   },
   "outputs": [],
   "source": [
    "for image, label in ds_train.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        aug_img = img_augmentation(tf.expand_dims(image, axis=0))\n",
    "        plt.imshow(aug_img[0].numpy().astype(\"uint8\"))\n",
    "        plt.title(\"{}\".format(format_label(label)))\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYKiecNfJrz1"
   },
   "source": [
    "### Prepare inputs\n",
    "\n",
    "Once we verify the input data and augmentation are working correctly,\n",
    "we prepare dataset for training. The input data are resized to uniform\n",
    "`IMG_SIZE`. The labels are put into one-hot\n",
    "(a.k.a. categorical) encoding. The dataset is batched.\n",
    "\n",
    "Note: `prefetch` and `AUTOTUNE` may in some situation improve\n",
    "performance, but depends on environment and the specific dataset used.\n",
    "See this [guide](https://www.tensorflow.org/guide/data_performance)\n",
    "for more information on data pipeline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MsFPzOEwJrz1"
   },
   "outputs": [],
   "source": [
    "# One-hot / categorical encoding\n",
    "def input_preprocess(image, label):\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    input_preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "ds_train = ds_train.batch(batch_size=batch_size, drop_remainder=True)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(input_preprocess)\n",
    "ds_test = ds_test.batch(batch_size=batch_size, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 224, 224, 3), (64, 120)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 600\n",
    "img_width = 600\n",
    "IMG_SIZE = (600, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_dir = pathlib.Path(\"/home/cdsw/dataset/train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4490 files belonging to 3 classes.\n",
      "Using 3592 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 600, 600, 3), (None,)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A1P7QpyJrz1"
   },
   "source": [
    "## Training a model from scratch\n",
    "\n",
    "We build an EfficientNetB0 with 120 output classes, that is initialized from scratch:\n",
    "\n",
    "Note: the accuracy will increase very slowly and may overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qzqg8ZPtJrz2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strategy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d0ad0572628a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEfficientNetB0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'strategy' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "with strategy.scope():\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = img_augmentation(inputs)\n",
    "    outputs = EfficientNetB0(include_top=True, weights=None, classes=NUM_CLASSES)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs = 1  # @param {type: \"slider\", min:10, max:100}\n",
    "hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzyIgZiNJrz2"
   },
   "source": [
    "Training the model is relatively fast (takes only 20 seconds per epoch on TPUv2 that is\n",
    "available on Colab). This might make it sounds easy to simply train EfficientNet on any\n",
    "dataset wanted from scratch. However, training EfficientNet on smaller datasets,\n",
    "especially those with lower resolution like CIFAR-100, faces the significant challenge of\n",
    "overfitting.\n",
    "\n",
    "Hence training from scratch requires very careful choice of hyperparameters and is\n",
    "difficult to find suitable regularization. It would also be much more demanding in resources.\n",
    "Plotting the training and validation accuracy\n",
    "makes it clear that validation accuracy stagnates at a low value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJnwvK5GJrz2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlFS_Um6Jrz2"
   },
   "source": [
    "## Transfer learning from pre-trained weights\n",
    "\n",
    "Here we initialize the model with pre-trained ImageNet weights,\n",
    "and we fine-tune it on our own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5voTzEXJrz2"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "\n",
    "def build_model(num_classes):\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = img_augmentation(inputs)\n",
    "    model = EfficientNetB0(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = False\n",
    "\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.2\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "    # Compile\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yos5nX-LJrz2"
   },
   "source": [
    "The first step to transfer learning is to freeze all layers and train only the top\n",
    "layers. For this step, a relatively large learning rate (1e-2) can be used.\n",
    "Note that validation accuracy and loss will usually be better than training\n",
    "accuracy and loss. This is because the regularization is strong, which only\n",
    "suppresses training-time metrics.\n",
    "\n",
    "Note that the convergence may take up to 50 epochs depending on choice of learning rate.\n",
    "If image augmentation layers were not\n",
    "applied, the validation accuracy may only reach ~60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uCUqNldJrz2"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = build_model(num_classes=NUM_CLASSES)\n",
    "\n",
    "epochs = 25  # @param {type: \"slider\", min:8, max:80}\n",
    "hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test, verbose=2)\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRTNptExJrz2"
   },
   "source": [
    "The second step is to unfreeze a number of layers and fit the model using smaller\n",
    "learning rate. In this example we show unfreezing all layers, but depending on\n",
    "specific dataset it may be desireble to only unfreeze a fraction of all layers.\n",
    "\n",
    "When the feature extraction with\n",
    "pretrained model works good enough, this step would give a very limited gain on\n",
    "validation accuracy. In our case we only see a small improvement,\n",
    "as ImageNet pretraining already exposed the model to a good amount of dogs.\n",
    "\n",
    "On the other hand, when we use pretrained weights on a dataset that is more different\n",
    "from ImageNet, this fine-tuning step can be crucial as the feature extractor also\n",
    "needs to be adjusted by a considerable amount. Such a situation can be demonstrated\n",
    "if choosing CIFAR-100 dataset instead, where fine-tuning boosts validation accuracy\n",
    "by about 10% to pass 80% on `EfficientNetB0`.\n",
    "In such a case the convergence may take more than 50 epochs.\n",
    "\n",
    "A side note on freezing/unfreezing models: setting `trainable` of a `Model` will\n",
    "simultaneously set all layers belonging to the `Model` to the same `trainable`\n",
    "attribute. Each layer is trainable only if both the layer itself and the model\n",
    "containing it are trainable. Hence when we need to partially freeze/unfreeze\n",
    "a model, we need to make sure the `trainable` attribute of the model is set\n",
    "to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZLlIrt-Jrz2"
   },
   "outputs": [],
   "source": [
    "\n",
    "def unfreeze_model(model):\n",
    "    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n",
    "    for layer in model.layers[-20:]:\n",
    "        if not isinstance(layer, layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "\n",
    "unfreeze_model(model)\n",
    "\n",
    "epochs = 10  # @param {type: \"slider\", min:8, max:50}\n",
    "hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test, verbose=2)\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZJ3hT_pJrz3"
   },
   "source": [
    "### Tips for fine tuning EfficientNet\n",
    "\n",
    "On unfreezing layers:\n",
    "\n",
    "- The `BathcNormalization` layers need to be kept frozen\n",
    "([more details](https://keras.io/guides/transfer_learning/)).\n",
    "If they are also turned to trainable, the\n",
    "first epoch after unfreezing will significantly reduce accuracy.\n",
    "- In some cases it may be beneficial to open up only a portion of layers instead of\n",
    "unfreezing all. This will make fine tuning much faster when going to larger models like\n",
    "B7.\n",
    "- Each block needs to be all turned on or off. This is because the architecture includes\n",
    "a shortcut from the first layer to the last layer for each block. Not respecting blocks\n",
    "also significantly harms the final performance.\n",
    "\n",
    "Some other tips for utilizing EfficientNet:\n",
    "\n",
    "- Larger variants of EfficientNet do not guarantee improved performance, especially for\n",
    "tasks with less data or fewer classes. In such a case, the larger variant of EfficientNet\n",
    "chosen, the harder it is to tune hyperparameters.\n",
    "- EMA (Exponential Moving Average) is very helpful in training EfficientNet from scratch,\n",
    "but not so much for transfer learning.\n",
    "- Do not use the RMSprop setup as in the original paper for transfer learning. The\n",
    "momentum and learning rate are too high for transfer learning. It will easily corrupt the\n",
    "pretrained weight and blow up the loss. A quick check is to see if loss (as categorical\n",
    "cross entropy) is getting significantly larger than log(NUM_CLASSES) after the same\n",
    "epoch. If so, the initial learning rate/momentum is too high.\n",
    "- Smaller batch size benefit validation accuracy, possibly due to effectively providing\n",
    "regularization.\n",
    "\n",
    "## Using the latest EfficientNet weights\n",
    "\n",
    "Since the initial paper, the EfficientNet has been improved by various methods for data\n",
    "preprocessing and for using unlabelled data to enhance learning results. These\n",
    "improvements are relatively hard and computationally costly to reproduce, and require\n",
    "extra code; but the weights are readily available in the form of TF checkpoint files. The\n",
    "model architecture has not changed, so loading the improved checkpoints is possible.\n",
    "\n",
    "To use a checkpoint provided at\n",
    "[the official model repository](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet), first\n",
    "download the checkpoint. As example, here we download noisy-student version of B1:\n",
    "\n",
    "```\n",
    "!wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet\\\n",
    "       /noisystudent/noisy_student_efficientnet-b1.tar.gz\n",
    "!tar -xf noisy_student_efficientnet-b1.tar.gz\n",
    "```\n",
    "\n",
    "Then use the script [efficientnet_weight_update_util.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/efficientnet_weight_update_util.py) to convert ckpt file to h5 file.\n",
    "\n",
    "```\n",
    "!python efficientnet_weight_update_util.py --model b1 --notop --ckpt \\\n",
    "        efficientnet-b1/model.ckpt --o efficientnetb1_notop.h5\n",
    "```\n",
    "\n",
    "When creating model, use the following to load new weight:\n",
    "\n",
    "```python\n",
    "model = EfficientNetB1(weights=\"efficientnetb1_notop.h5\", include_top=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "image_classification_efficientnet_fine_tuning",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
